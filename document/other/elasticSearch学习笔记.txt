elasticSearch学习笔记

一. 安装
    1. Linux
    下载tar文件,解压缩, 进入bin目录,使用./elasticsearch启动.
    启动过程中,如果出现已杀死, 是内存太小了, 可以调大ES_JAVA_OPTS="-Xms512m -Xmx512m"
    linux不能用root用户启动,可以添加参数启动, ./elasticsearch -Des.insecure.allow.root=true, 或者直接修改文件elasticsearch, 加上
ES_JAVA_OPTS="-Des.insecure.allow.root=true"
    2. windows
    直接下载压缩包, 解析后, 进入bin目录执行elasticsearch.bat就启动成功了, 默认的商品为9200.
    注意: 运行需要JDK环境
二. CURL命令
    curl www.baidu.com 通过命令访问一个网页
    curl -v www.baidu.com 显示一次请求的通讯过程
    curl -X GET/POST/PUT/DELETE url 操作某种类型的请求

三. 配合使用的软件
    elasticSearch-head. Kibana. 都是提供elasticSearch的可视化操作服务.

四. 倒排索引
     语义分词, 建立倒排索引(某单词在某文档出现的次数, 每次出现的位置), 查找.
     es在建立倒排索引的时候,为了提高搜索准确度,会使用一个标准化的规则, 又如忽略大小写, 忽略单复数等.

五. 分词器
     进行语义分析, 转换的. 分词器包括3部分:
     1. character filter 分词之前的预处理, 过滤html标签等
     2. tokenizer: 分词
     3. token filter: 标准化

     内置分词器:
     standard 分词器(默认的)他会将词汇单元转换成小写形式,并去掉停用词和标点符号 , 支持中文, 采用的方法为单字切分.
     simple 分词器, 首先会通过非字母字符来分割文本信息,然后将词汇单元统一为小写形式, 该分析器会去掉数字类型的字符.
     whitespace 分词器, 仅仅是去除空格, 对字符没有lowcase化, 不支持中文, 并且不对生成的词汇单元进行其它标准化处理.
     language 分词器, 特定语言的分词器, 不支持中文.

     安装中文分词器:

六. 基于kibana操作ES
    创建一个索引:
    PUT /lib/                                 restful基于put请求发送数据, 指定了创建索引("lib")
    {
      "settings": {
        "index":{
          "number_of_shards":3,               数据分片为3份
          "number_of_replicas":0              因为只有一个节点,备份设置为0
        }
      }
    }

    PUT lib2                                  这也是创建索引,简写, 各属性用默认值

    GET /lib/_settings                        查看索引信息, 以"_"开头的一般是ES默认创建好的.

    GET _all/_settings                        查看所有索引配置

    PUT /lib/user/1                           指定id添加文件, 1为id,根据这个查找文档
    {
      "first_name":"tian",
      "last_name":"xiong",
      "age":30,
      "about":"i like to collect rock.",
      "interests":["music"]
    }

   POST /lib/user/                          不指定id,则用post请求, id会由系统生成
   {
     "first_name":"chen",
     "last_name":"haiying",
     "age":25,
     "about":"i love tianxiong.",
     "interests":["music"]
   }

   GET /lib/user/1                          获取指定id文档

   GET /lib/user/1?_source=first_name,last_name   只获取first_name和last_name两个字段值

   POST /lib/user/1/_update                 显示指定修改指定文档指定属性的值. 也可以用put进行覆盖更新
   {
     "doc":{
       "age":18
     }
   }

   DELETE /lib/user/3                      删除某个文档
   DELETE lib2                             删除索引

七. 批量获取
   GET /_mget                              _mget 是批量获取的语法
   {
     "docs":[
       {
         "_index":"lib",
         "_type":"user",
         "_id":1
       },
       {
         "_index":"lib",
         "_type":"user",
         "_id":2
       },
       {
         "_index":"lib",
         "_type":"user",
         "_id":3
       }
       ]
   }

   GET /lib/user/_mget                      当索引和类型相同时, 还可以把这两个写在路径上, 下面json中就不用再单独指定
   {
     "docs":[
       {
         "_id":1
       },
       {
         "_id":2
       }
       ]
   }

   GET /lib/user/_mget                     更简单的写法
   {
     "ids":[1,2]
   }

八. 使用Bulk API实现批量操作
    bulk的格式:
    {action:{metadata}}
    {requstbody}
    action:(行为)
    create:文档不存在时创建
    update:更新文档
    index:创建新文档或替换已有文档
    delete; 删除一个文档
    metadata: _index,_type,_id
    create和index的区别: 如果数据存在, 使用create操作失败, 会提示文档已经存在,使用index则可以成功执行.

九. 版本控制
    ES使用了乐观锁保证数据一致性, 也就是说, 当用户对文档进行操作时, 并不需要加锁和解锁操作, 只需要指定要操作的版本即可.
    当版本号一致时, ES会允许该操作顺利执行,当版本有冲突时,会提示并拋出异常.

    POST /lib/user/1/_update?version_type=1    在更新数据时, 通过version_type=1指定了文档版本为1
    {
      "doc":{
        "age":20
      }
    }

    如果当前版本为1,则更新成功.否则,返回异常.
    也可以通过version指定版本操作. 只要该版本号大于当前版本号就可以了

十. mapping
   一是规定了字段的数据类型
   支持的属性:
        "store": false 是否单独设置此字段的是否存储而从_source字段中分离, 默认false,只能搜索, 不能获取值
        "index":true   分词, 设置false,字段将不会被索引.
        "analyzer": "ik"  指定分词, 默认用的是standard
        "boost":1.23   字段级别的分数加权, 默认是1.0
        "doc_values":false  对不分词的字段, 默认都是开启的, 分词字段不能用. 节约内存
        "ignore_above":100  超过多少长度的进行忽略, 默认256
        ......

十一. 手动创建mapping
   PUT /lib2
   {
     "settings":{
       "number_of_replicas": 0,
       "number_of_shards": 3
     },
     "mappings": {
       "books":{
         "properties":{
           "title":{
             "type":"text"
           },
           "name":{
             "type":"text",
             "analyzer":"standard"            指定分词器
           },
           "publish_date":{
             "type":"date",
             "index":false                    显示指定该字段不创建倒排索引.
           },
           "price":{
             "type":"double"
           },
           "number":{
             "type":"integer"
           }
         }
       }
     }
   }

十二. 基本查询(query查询重点)
    例: GET /myindex/article/_search?q=title:java
       请求方式  文档   _search表示查询操作  q后面跟查询条件  title是文档中的一个属性 : 表示用查询, 后面是查的内容

       GET /myindex/article/_search?q=content:java&sort=author_id:asc
       &表示条件并且, 根据author_id字段进行升序排序

       GET /myindex/article/_search       term 查询字段content, 有内容python的文档
       {
         "query":{
           "term": {
             "content": {
               "value": "python"
             }
           }
         }
       }


       GET /myindex/article/_search    terms 查询字段content, 有内容python或者php的文档
       {
         "query":{
           "terms": {
             "content": [
               "python",
               "php"
             ]
           }
           }
         }


         GET /myindex/article/_search
         {
           "from": 0,                  指定开始查找文档
           "size": 1,                  指定返回一个 , 结合上面参数可以实现分页
           "version": true,            返回文档版本号
           "query":{
             "terms": {
               "content": [
                 "python",
                 "php"
               ]
             }
             }
           }


        GET /myindex/article/_search   match会把传入的条件通过分词器分解, 再查询, 也就是内容含有php或者best都会查出来.
        {
          "query": {
            "match": {
              "content": "php best"
            }
          }
        }

        GET /myindex/article/_search   title和content任意一个字段中包含有php就会查出
        {
          "query": {
            "multi_match": {
              "query": "php",
              "fields": ["title","content"]
            }
          }
        }

        GET /myindex/article/_search    match_phrase表示短语匹配, 也就是指定字段中要含有完全一样的内容才查出.
        {
          "_source":["title","content"],    指定返回哪些字段.这里指定字段时, 可以用*通配符
          "query": {
            "match_phrase": {
              "content": "python php"
            }
          }
        }

        GET /myindex/article/_search   range可以查询范围, 还支持数字类型的
        {
          "query": {
            "range": {
              "post_date": {
                "gte": "2019-05-01",
                "lte": "2019-06-10"
              }
            }
          }
        }

        GET /myindex/article/_search   wildcard类型于正则匹配查询
        {
          "query":{
            "wildcard": {
              "title": {
                "value": "ph?"
              }
            }
          }
        }

        GET /myindex/article/_search   fuzzy相似度查询,这里的pythion和python虽然不一样, 但是很相似,所以也能查出来.
        {
          "query":{
            "fuzzy": {
              "content": "pythion"
            }
          },
           "highlight": {             highlight可以指定对返回的结果进行高亮显示, 需要注意的是返回的json和原始文档不一样,
                                               高亮部分字段会有其它嵌套属性进行标识.
              "fields":{
                "content":{}
              }
            }
        }

十三. filter查询(速度比query查询快, 可以缓存)

    GET /lib4/items/_search
    {
      "query": {
        "bool": {
          "filter": {
            "term":{
              "price":40              单个过滤, 价格为40的
            }
          }
        }
      }
    }

    GET /lib4/items/_search
    {
      "query": {
        "bool": {
          "filter": {
            "terms":{
              "price":[20,40]       过滤价格为20或者40的
            }
          }
        }
      }
    }

 十四. bool过滤查询
    可以实现组合过滤查询.
    格式: {"bool":{"must":[],"should":[],"must_not":[]}}
    must必须满足类似于sql中的and, should类型于sql中的or, must_not不能满足
    exists 表示过滤某个字段为为空

 十五. 聚合查询
   GET /lib4/items/_search
   {
     "aggs": {                  aggs 固定写法
       "price_of_sum": {        price_of_sum类似数据库别名
         "sum": {               sum,min, max, avg等函数. cardinality类型于count(distinct)
           "field": "price"
         }
       }
     }
   }

  十六, ES原理,特点
   ES是一个分布式系统,隐藏了复杂的处理机制.
   1. 分布式架构的透明隐藏机制.
   分片机制,  我们不用关心数据是按什么机制分片的, 最后放入到哪个分片中.
   分片的副本, 集群发现机制, 比如当我们启动了一个ES进程, 当启动第二个ES进程时, 这个进程做为一个node, 自动就发现了集群
        并且加入进去.
   shard负载均衡, 比如现有10 shard, 集群中有3个节点, ES会进行均衡的进行分配, 以保持每个节点为的均衡负载请求.
   2. 扩容机制
     垂直扩容, 购置新机器, 替换已有机器.
     水平扩容, 直接增加节点.
   3. rebalance
     增加或者减少节点会自动均衡
   4. master 节点
     主节点的主要职责是和集群操作相关的内容, 如创建或删除索引, 跟踪那些节点是集群的一部分, 并决定哪些分片分配给相关的
     节点, 稳定的主节点是对集群的健康非常重要的.
   5. 节点对等
     每个节点都能接受请求, 每个节点接收到请求后都能把该请求路由到有相关数据的其它节点上, 接收原始请求的节点负责采集数据
     并返回给客户端.
   6. 分片和副本机制.
     index包含多个shard
     每个shard都是一个最小工作单元. 承载部分数据, 每个shard都是一个lucene实例, 有完整的建立索引处理请求的能力.
     增减节点时, shard会自动在nodes中负载均衡.
   7. 水平扩容过程
   8. 容错性
     可接受宕机的台数越多, 容错性越强, 基本原理是对shard做多个备份实现.
     a. 重新选举另外一个节点做为master.
     b. master会把丢失的primary shard的其中一个副本提升为primary shard.此时,所有的primary shard都是活跃的.
     c. 把宕机的服务器重启, master会把每个primary shard上的数据复制一份到该服务器上.

  十七. 文档核心原数据分析
    1._index
        说明了一个文档存储在哪个索引中.
        同一个索引下存放的相似的文档(文档的field多数是相同的)
        索引名必须是小写的, 不能以下划线开头, 不能包括逗号.
    2. _type
        表示文档属于索引中的哪个类型
        一个索引下只能有一个type
        类型名可以是大写也可以是小写的, 不能以下划线开头, 不能包含逗号
    3. _id
        文档的唯一标识, 和索引, 类型组合在一起唯一标识了一个文档.
        可以手动指定值, 也可以由es来生成这个值.
    4. id的生成方式,
       a. 手动指定, 通常是数据库中有,则直接导入
       b. 如果数据本身没有, 则让es生成.es生成的id是长为20的字符串(GUID方式生成, 保证分布式生成不冲突)

  十八. _source原数据
    1. 默认情况下_source返回的是所有字段的值,可以在查询时通过_source后跟指定的字段

  十九. 改变文档的两种方式
    1. put 替换新的
    2. post 只改部分.
    3. post方式比put方式网络数据传输的次数要少, 从而提高了性能, post方式从查询文档修改文档,再到创建新文档都在es内部完成.
    4. post方式发生并发冲突的可能性要小
    5. 删除也是先把文档标记成已经删除,在合适的时机才真正删除

  二十. es支持脚本方式操作文档

  二十一. 文档路由原理分析
    1. 文档路由到分片上:
       一个索引由多个分片构成,当添加,删除,修改一个文档时, es就需要决定这个文档存储在哪个分片上,这个过程就称为数据路由.
       示例: 一个索引, 3个primary shard
       a. 每次修改时, 都有一个routing值, 默认是文档的id值
       b. 对这个routing值使用hash进行计算
       c. 计算出的值再和主分片个数取余.

  二十二. 写一致性原理以及quorum机制
    1. 任何一个参数增删改操作都可以跟上一个参数consistency可以给该参数指定的值
       one: 只要有一个primary shard是活跃的就可以执行
       all: 所有的primary shard和replica shard都是活跃的才能执行
       quorum: 默认值, 大部分shard是活跃的才能执行

  二十三. 文档查询的内部原理
    1. 查询请求发送给任意节点, 该节点就成了coordinating node, 该节点使用路由算法算出文档所在的primary shard
    2. 协调节点把请求转发给primary shard也可以转发给replica shard(使用轮询调度算法)
    3. 处理请求的节点把结果返回给协调节点, 协调节点再返回给应用程序.
    特殊情况: 请求的文档还在建立索引的过程中, primary shard存在, replica shard不存在, 但是请求被转发到了replica shard上,
    就会出现找不到文档.

  二十四. bulk批量操作的json格式解析
    bulk格式:
    {action:{metadata}}\n
    {requestbody}\n

  二十五. 多索引,多类型查询
    GET /lib,lib4/_search 查询多个索引下所有文档
    GET /*2,*4/_search 支持用通配符

  二十六. 分页查询中的deep paging问题
        deep paging 查询很深, 比如一个索引有三个primary shard, 分别存储了6000条数据, 我们要得到第100页数据(每页10条), 类似这
    种情况就叫deep paging问题.
        如何得到10条数据?
        错误的做法: 在每个shard中搜索990到999这10条记录, 然后用这30条数据进行排序,取10条, 这种做法不对. 因为3个shard中的数据
    _score分数不一样, 可能某一个shard中第一条数据的_score分数比另一个shard中的第1000条都要高.
        正确的做法: 每个shard把0到999条数据全部搜索出来, 然后全部传送给coordinate node, 由coordinate node节点按_score排序后,
    取出第100页的10条数据, 然后返回给客户端.
        deep paging性能问题,
        1. 耗费网络宽带, 因为搜索过深的话, 各shard要把数据传送给coordinate node,这个过程是有大量数据传输的, 消耗网络.
        2. 消耗内存, 各shard把数据传送给coordinate node,这个传递回来的数据是保存在内存中的.需要大量内存.
        3. 消费cpu, 大量数据进行排序需要cpu.
        由于上面几个原因应该尽量避免deep paging问题.

  二十七. query string 和 copy to
       copy to是在创建_mapping时可以指定对某些字段创建copy to(只能针对文本字段),当一个字段创建了copy to,就会把一条记录中的
       多个字段的内容合并到copy to指定的字段中.

  二十八. 字符串排序问题
       默认情况下对于字符串类型的字段进行排序通常不准确,因为已经被分词成多个词条了.
       解决方式: 对字段建立两个索引, 一个倒排索引用于查询.一个下拜索引用于排序

  二十九. 如何计算相关度分数.